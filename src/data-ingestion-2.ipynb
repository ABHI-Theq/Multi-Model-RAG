{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a811ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import fitz\n",
    "import torch\n",
    "from langchain_community.document_loaders.parsers import PyMuPDFParser\n",
    "from langchain_core.documents.base import Blob\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import CLIPProcessor,CLIPModel\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5800af41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "116d4f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c2f9ec128c4c8bb1960a428b49e57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SWEETY\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SWEETY\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baeceef5c64849cd877a1d425a2b7df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4dcac1abe984f5cb3bd896c98bd7326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f803dd04c7f41b0865b35b4d2f610e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89453986a45844f9beaa36ec071fc52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6188e060336243c798ce99d73a24dc36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b85d65ac5c3456c81610fbfa6dcb403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314bef0e496f486694c9fdcc064bd5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eaca76c38b14566a0ac4d0a17adee55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path=\"../data/attention-is-all-you-need-Paper.pdf\"\n",
    "\n",
    "model=CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor=CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=30\n",
    ")\n",
    "docs=fitz.open(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "08c653c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text:str):\n",
    "    inputs=processor(\n",
    "        text=text,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=80\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        features=model.get_text_features(**inputs)\n",
    "\n",
    "        #normalize embeddings\n",
    "        features=features/features.norm(dim=-1,keepdim=True)\n",
    "        return features.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e255879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_image(image_data):\n",
    "    if isinstance(image_data,str):\n",
    "        image=Image.Open(image_data).convert(\"RGB\")\n",
    "    else: \n",
    "        image=image_data\n",
    "\n",
    "    inputs=processor(images=image,return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features=model.get_image_features(**inputs)\n",
    "\n",
    "        features=features/features.norm(dim=-1,keepdim=True)\n",
    "\n",
    "        return features.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2327ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.page_count\n",
    "all_docs=[]\n",
    "all_embeddings=[]\n",
    "image_data_store={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be8eaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[(94, 95, 1520, 2239, 8, 'DeviceRGB', '', 'Im1', 'FlateDecode', 0)]\n",
      "[(102, 103, 835, 1282, 8, 'DeviceRGB', '', 'Im3', 'FlateDecode', 0), (104, 105, 445, 884, 8, 'DeviceRGB', '', 'Im2', 'FlateDecode', 0)]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for i,page in enumerate(docs):\n",
    "    \n",
    "    ### processing text cotent of the page\n",
    "    text=page.get_text()\n",
    "    if text.strip():\n",
    "        temp_doc=Document(page_content=text,metadata={\"page\":i,\"type\":\"text\"})\n",
    "        text_chunks=splitter.split_documents([temp_doc])\n",
    "\n",
    "        for chunk in text_chunks:\n",
    "            all_docs.append(chunk)\n",
    "            embedding=embed_text(chunk)\n",
    "            all_embeddings.append(embedding)\n",
    "    \n",
    "\n",
    "    #### processing image content of the page\n",
    "\n",
    "    for img_i,img in enumerate(page.get_images(full=True)):\n",
    "        try:\n",
    "            xref=img[0]\n",
    "            base_image=docs.extract_image(xref)\n",
    "            image_bytes=base_image.get('image')\n",
    "\n",
    "            ###convert to PIL Image\n",
    "            pil_image=Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
    "\n",
    "            image_id=f\"page: {i}_img_{img_i}\"\n",
    "\n",
    "            #save the image bytes in base64 format for mdoel use\n",
    "            buffered=io.BytesIO()\n",
    "            pil_image.save(buffered,format=\"PNG\")\n",
    "            img_base64=base64.b64encode(buffered.getValue()).decode()\n",
    "            image_data_store[image_id]=img_base64\n",
    "\n",
    "            #now create embeddings of image and store\n",
    "\n",
    "            all_embeddings.append(embed_image(pil_image))\n",
    "            img_doc=Document(\n",
    "                page_content=f\"[Image: {image_id}]\",\n",
    "                metadata={\"page\":i,\"type\":\"image\",\"image_id\":image_id}\n",
    "            )\n",
    "            all_docs.append(img_doc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_i} on page {i}: {e}\")\n",
    "            continue   \n",
    "            \n",
    "\n",
    "docs.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba8b0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
